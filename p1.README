################################################################################

Group info:
agoel5 Anshuman Goel
kgondha Kaustubh G Gondhalekar
ndas Neha Das

################################################################################

Elapsed time A: 00.24 sec
Elapsed time B: 38.47 sec
Elapsed time C: 38.32 sec
Elapsed time D: 38.36 sec

NUMA architecture for an opteron processor with 16 cores and 2 NUMA nodes ( 8 cores per NUMA node )

 ___________________________________________
|                                           |
|      NUMA                     NUMA        |
|      node 0                   node 1      |
|      _____                    _____       |
|     |o  o |                  |o  o |      |
|     |o  o |                  |o  o |      |
|     |o  o |------------------|o  o |      |
|     |o  o |    interconnect  |o  o |      |
|     |_____|                  |_____|      |
|        |                        |         |
|     ___|__                   ___|__       |
|    |______|                 |______|      |
|                                           |
|    NUMA local                NUMA local   |
|    memory                    memory       |
|___________________________________________|

    Node 0

The above diagram explains NUMA architecture.
Reference ( http://dl.acm.org/citation.cfm?id=2500468.2500477&coll=portal&dl=ACM )

Each NUMA node has 8 processors and a local physical memory and each processor has its own local cache.
The NUMA nodes are connected through an interconnect.

Going forward, we have drawn conclusions assuming that the opteron processor
also follows a similar NUMA architecture.

Thus, if OpenMP threads are scheduled across different NUMA nodes,
the inter-cache line transfers will take more time due to NUMA interconnect delays.

Also, if a single MPI task allocates memory on both the NUMA memory nodes,
accessing elements of this memory will also introduce NUMA interconnect delays.
This factor could dominate the observed delays for configurations with large problem sizes.

For Configuration A (1,8,8,20,5):

@--- MPI Time (seconds) ---------------------------------------------------
Task    AppTime    MPITime     MPI%
   0      0.251    0.00719     2.86
   1      0.248     0.0217     8.77
   2      0.241      0.033    13.68
   3      0.241     0.0443    18.35
   4      0.243     0.0475    19.50
   5      0.239     0.0423    17.73
   6      0.243     0.0268    11.02
   7      0.239     0.0392    16.38
   *       1.95      0.262    13.46

Most expensive MPI call:
Call                 Site       Time    App%    MPI%     COV
Allreduce              12        211   10.86   80.67    0.52

In comparison with other configurations, we observe that this configuration is
the fastest among all. The most probable reason is because all the OpenMP threads
are core binded and all the MPI calls are within the same node (data
transfers are contained within the node's NIC). Thus, it reduces the communication latency.
If the problem size is quite large such that Operating System isn't able to
allocate memory for the task in the same physical memory of the NUMA node, it will allocate
it in physical memory of the other NUMA node. This may incur interconnect delays between the NUMA nodes.
However, in this case it is observed that the NUMA effects are very less since, the observed
execution time is less than the configuration B.

For Configuration B (1,8,8,20,5) without core binding:

\@--- MPI Time (seconds) ---------------------------------------------------
---------------------------------------------------------------------------
Task    AppTime    MPITime     MPI%
   0       38.5     0.0969     0.25
   1       38.5      0.166     0.43
   2       38.5      0.273     0.71
   3       38.5      0.164     0.43
   4       38.5       0.15     0.39
   5       38.5      0.127     0.33
   6       38.5      0.172     0.45
   7       38.5      0.122     0.32
   *        308       1.27     0.41

Most expensive MPI call:
Call                 Site       Time    App%    MPI%     COV
Allreduce              12        465    0.15   36.59    0.47

MPI tasks are pinned to the cores by default. Since core binding is turned off for OpenMP threads,
they can get scheduled at any of the 16 cores by the operating system.
If an OpenMP thread requests a buffer which was previously present on some other core's L1 cache,
the current core will have to copy the entire buffer into its L1 cache.
Moreover if two threads are in different NUMA nodes, cache line transfer delays increase by a huge margin due to NUMA interconnect delays.
This can happen multiple times and possibly lead to an increased execution time than the configuration
with binding enabled (configuration A).


For Configuration C (4,0,64,80,20):

@--- MPI Time (seconds) ---------------------------------------------------
---------------------------------------------------------------------------
Task    AppTime    MPITime     MPI%
   0       38.6      0.412     1.07
   1       38.6      0.335     0.87
   2       38.5       2.76     7.17
   3       38.5       2.83     7.36
   4       38.4       4.62    12.03
   5       38.4       4.75    12.37
   6       38.4       5.34    13.92
   7       38.4       5.26    13.71
   8       38.5       6.39    16.60
   9       38.3       6.96    18.18
  10       38.3       7.09    18.52
  11       38.5       3.01     7.82
  12       38.5       1.79     4.65
  13       38.5       2.18     5.65
  14       38.5       3.11     8.10
  15       38.4       3.55     9.24
  16       38.6       4.12    10.66
  17       38.4          5    13.02
  18       38.4       5.21    13.60
  19       38.4          5    13.02
  20       38.5       7.52    19.50
  21       38.3       6.69    17.48
  22       38.5       1.71     4.45
  23       38.5       2.77     7.19
  24       38.5       2.17     5.64
  25       38.5       2.67     6.94
  26       38.5       3.52     9.15
  27       38.4       4.98    12.99
  28       38.3       5.43    14.15
  29       38.3       6.11    15.96
  30       38.3       6.83    17.85
  31       38.3       6.74    17.62
  32       38.6       7.33    18.96
  33       38.5       2.18     5.66
  34       38.6       1.67     4.32
  35       38.5          3     7.79
  36       38.6       1.65     4.28
  37       38.5       3.19     8.28
  38       38.4        3.9    10.15
  39       38.4       4.75    12.37
  40       38.3       6.28    16.39
  41       38.3       6.61    17.26
  42       38.3       6.64    17.34
  43       38.3       7.29    19.06
  44       38.6      0.455     1.18
  45       38.6      0.395     1.02
  46       38.5       2.21     5.73
  47       38.5       2.06     5.36
  48       38.6       3.34     8.66
  49       38.4       5.13    13.37
  50       38.4       5.36    13.98
  51       38.4       4.44    11.58
  52       38.4       7.37    19.17
  53       38.2       7.33    19.15
  54       38.2        7.4    19.34
  55       38.4       3.63     9.43
  56       38.5       1.83     4.74
  57       38.5       2.11     5.48
  58       38.5       2.88     7.49
  59       38.5       2.95     7.67
  60       38.3       5.63    14.70
  61       38.3       6.06    15.83
  62       38.3       6.03    15.74
  63       38.3       7.09    18.54
   *   2.46e+03        275    11.18

Most expensive MPI call:
Call                 Site       Time    App%    MPI%     COV
Allreduce               8   2.53e+05   10.29   92.05    0.53

In configuration C, the problem size, number of tasks, and number of nodes increases.
This leads to increased MPI Send/Recv calls over the network between nodes.
A larger problem size causes greater NUMA interconnect delays as discussed in the NUMA architecture diagram above.
Since the number of OpenMP threads are zero, there are no cache line transfers due to threads.

Therefore, the delays introduced due to large number of MPI calls and the
interconnect delays dominate the observed execution time.

For Configuration D (4,0,64,80,20) without core-binding:

@--- MPI Time (seconds) ---------------------------------------------------
---------------------------------------------------------------------------
Task    AppTime    MPITime     MPI%
   0       38.6      0.485     1.26
   1       38.6      0.339     0.88
   2       38.6       3.01     7.79
   3       38.5       2.83     7.35
   4       38.4       4.58    11.92
   5       38.4       4.86    12.66
   6       38.4       5.52    14.39
   7       38.5        5.5    14.28
   8       38.6       6.67    17.28
   9       38.4       7.19    18.71
  10       38.4       7.22    18.83
  11       38.4       3.11     8.09
  12       38.5       1.67     4.32
  13       38.5       2.22     5.77
  14       38.6       3.27     8.46
  15       38.5       3.32     8.62
  16       38.8       4.17    10.77
  17       38.5        5.2    13.50
  18       38.4       4.88    12.71
  19       38.5       5.08    13.18
  20       38.6       7.58    19.62
  21       38.3        6.9    18.00
  22       38.5       1.79     4.64
  23       38.5        2.8     7.28
  24       38.7       2.26     5.84
  25       38.5       2.65     6.89
  26       38.6       3.66     9.48
  27       38.4       4.81    12.54
  28       38.4       5.45    14.20
  29       38.3       6.11    15.95
  30       38.3       6.59    17.21
  31       38.3       6.59    17.21
  32       38.6       7.33    18.99
  33       38.6       2.24     5.79
  34       38.6       1.71     4.42
  35       38.4       3.33     8.66
  36       38.6       1.72     4.45
  37       38.5       3.13     8.13
  38       38.4       3.88    10.10
  39       38.5        4.9    12.73
  40       38.3        6.3    16.46
  41       38.3       6.49    16.93
  42       38.4       6.68    17.38
  43       38.4       7.23    18.84
  44       38.8      0.637     1.64
  45       38.6      0.339     0.88
  46       38.6       2.21     5.72
  47       38.5       2.06     5.35
  48       38.7       3.43     8.87
  49       38.5       5.27    13.70
  50       38.5       5.67    14.71
  51       38.5       4.65    12.06
  52       38.5       7.32    19.03
  53       38.4        7.4    19.28
  54       38.4       7.71    20.07
  55       38.5       3.58     9.32
  56       38.6       1.91     4.96
  57       38.7       2.28     5.91
  58       38.6       2.87     7.45
  59       38.5       2.94     7.64
  60       38.4       5.59    14.57
  61       38.4       6.13    15.98
  62       38.4       6.07    15.82
  63       38.4        7.4    19.28
   *   2.46e+03        279    11.31

Most expensive MPI call:
Call                 Site       Time    App%    MPI%     COV
Allreduce               8   2.54e+05   10.30   91.02    0.53

Since there are zero OpenMP threads, this configuration is same as configuration C.
Also, the observed time is nearly same as the configuration C which supports our line of reasoning.

################################################################################

Parallelism:

We observe that each configuration has same amount of parallelism (in total 64 task+threads)
but different type of parallelism. However, configurations A and B have an
8(process)*8(threads) structure and configuration C and D have 64(process)*1(thread).
For configuration A and B, there will be greater level of implicit communication
due to shared memory among different threads of the same process.
For configuration C and D, every process has its own virtual memory address space
with greater memory overhead. This will increase explicit communication among all
processes due to MPI Send and Receive calls.

################################################################################

Looking at the percentage of time spent in MPI for each of the configurations,
we observe that configurations A and B, task 0 contributes the least towards MPI calls.
This could mean that the task 0 act as a master delegating the computation/communication task to other tasks.
However, we could not find this pattern for configuration C and D.
We speculate that for multi-node configuration, there could be multiple masters (possibly one at each node).

We observed most of the time is spent in MPI_AllReduce for all the configurations.

Limitations of Parallelism:
Divisibility of Problem Size: For a fixed problem size, increase in the number
of threads beyond a certain limit doesn't provide any significant
performance benefits. In fact, after a limit the performance deteriorates due to thrashing.
